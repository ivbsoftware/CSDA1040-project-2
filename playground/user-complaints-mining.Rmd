---
title: "User Complaints Mining"
author: "Igor Baranov, Michael Parravani, Ariana Biagi, Hui Fang Cai"
date: "February 12, 2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("ggplot2")
library("readr")
library("RDSTK")
library("Matrix")
library("dplyr")
library("forcats")
library(tm)
library(stopwords)
library(caTools)
library(tidytext)
library(philentropy)

set.seed(777)
```

```{r wrap-hook, include=FALSE}
# Wrapping long text hook - https://github.com/yihui/knitr-examples/blob/master/077-wrap-output.Rmd
# See usage of print statement with linewidth atrribute in the chunk header.
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Seting directory to current script (works in R Studio only)
#this.dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
#setwd(this.dir)
```
Introduction 

Customer complaints give businesse valuable information about how they need to improve. This project uses Text Mining techniques to  extract useful, interesting and hidden information from consumers'narrative complaints from Consumer Financial Protection Bureau. This might be used to help financial institutions improve the quality of their services and enchance their competitive capacity in the market.

The data used in this project can be downloaded from Kaggle's [Consumer Complaints Database -Financial Services complaints ](https://www.kaggle.com/sebastienverpile/consumercomplaintsdata/downloads/Consumer_Complaints.csv).
(81,940 rows x 18 columns) In this project,all of no complaints (missing values) and unnecssary columns are removed for the analysis. 

# Preparing data
## Load and clean the customer complaints data

Loading and removing rows with no complaint narrative and unnecessary columns:
```{r message=FALSE, warning=FALSE}
if(!file.exists("./user-complaints-mining/df.Rds")) {
  df <- read_csv(file="../data/Consumer_Complaints.csv.zip",col_names = TRUE)
  df <- df[,-c(1,7,9:18)]
  df <- df[!is.na(df[,"Consumer complaint narrative"]),] #199,970
  df <- df[!is.na(df[,"Company"]),] # no NA's
  df <- df[!is.na(df[,"Product"]),] # no NA's
  df <- df[!is.na(df[,"Issue"]),] # no NA's
  df <- df[!is.na(df[,"Sub-product"]),] # 147,788 total left
  df <- df[!is.na(df[,"Sub-issue"]),]   # 81,940 total left
  
  # Converting all but narrative columns to factors
  df$Product <- as.factor(df$Product)
  df$`Sub-product` <- as.factor(df$`Sub-product`)
  df$Issue <- as.factor(df$Issue)
  df$`Sub-issue` <- as.factor(df$`Sub-issue`)
  df$Company <- as.factor(df$Company)
  saveRDS(df, file = "./user-complaints-mining/df.Rds")
  gc()
} else {
  df <- readRDS("./user-complaints-mining/df.Rds")
}
df
```

# Feature engineering

## Distribution of the most frequent "Issue" complaints

```{r fig.height=7, fig.width=5.5, message=FALSE, warning=FALSE}
full_sorted_issues_list <- levels(fct_infreq(df$Issue))
saveRDS(full_sorted_issues_list, file = "./user-complaints-mining/full_sorted_issues_list.Rds")

ggplot() + aes(fct_infreq(df[df$Issue %in% full_sorted_issues_list[1:30],]$Issue))+ 
  geom_histogram(colour="black", fill="white", stat = "count")+
  ylab("Issue Complaints Frequency") + xlab("")+
  theme(axis.text.x = element_text(angle =90, hjust = 1))
```
There is substantial drop off of use of different complaint codes
\newpage
## Distribution of the most frequent "Sub-issue" complaints

```{r fig.height=7.5, fig.width=5.5, message=FALSE, warning=FALSE}
most_freq_subissues_list <- levels(fct_infreq(df$`Sub-issue`))[1:30]
ggplot() + aes(fct_infreq(df[df$`Sub-issue` %in% most_freq_subissues_list,]$`Sub-issue`))+ 
  geom_histogram(colour="black", fill="white", stat = "count")+
  ylab("Sub-Issue Complaints Frequency") + xlab("")+
  theme(axis.text.x = element_text(angle =90, hjust = 1))
```
Again, fairly substantial drop off in use of subissue codes

\newpage
## Distribution of the most frequent "Product" complaints

```{r fig.height=7.5, fig.width=5.5, message=FALSE, warning=FALSE}
most_freq_product_list <- levels(fct_infreq(df$Product))[1:30]
ggplot() + aes(fct_infreq(df[df$Product %in% most_freq_product_list,]$Product))+ 
  geom_histogram(colour="black", fill="white", stat = "count")+
  ylab("Product Compliants Frequency") + xlab("")+
  theme(axis.text.x = element_text(angle =90, hjust = 1))
```

\newpage
## Distribution of the most frequent "Sub-product" complaints

```{r fig.height=7.5, fig.width=5.5, message=FALSE, warning=FALSE}
most_freq_subproduct_list <- levels(fct_infreq(df$`Sub-product`))
ggplot() + aes(fct_infreq(df[df$`Sub-product` %in% most_freq_subproduct_list,]$`Sub-product`))+ 
  geom_histogram(colour="black", fill="white", stat = "count")+
  ylab("Sub-Product Complaints Frequency") + xlab("")+
  theme(axis.text.x = element_text(angle =90, hjust = 1))
```

\newpage
## Distribution of the most frequent "Company" complaints

```{r fig.height=7.5, fig.width=5.5, message=FALSE, warning=FALSE}
most_freq_company_list <- levels(fct_infreq(df$Company))[1:30]
ggplot() + aes(fct_infreq(df[df$Company %in% most_freq_company_list,]$Company))+ 
  geom_histogram(colour="black", fill="white", stat = "count")+
  ylab("Company Complaints Frequency") + xlab("")+
  theme(axis.text.x = element_text(angle =90, hjust = 1))
```

\newpage

# Text Mining

## Split data into test and train sets

```{r}
issuesToPredict <- 12

df$issueId <- match(df$Issue, full_sorted_issues_list)
df_issues <- df[df$issueId <= issuesToPredict,]

set.seed(123)
sample = sample.split(df_issues$`Consumer complaint narrative`, SplitRatio = .5)
train_full = subset(df_issues, sample == TRUE)
train_full$oid <- c(1:nrow(train_full))
test  = subset(df_issues, sample == FALSE)
```

## Distribution of the text word count

```{r message=FALSE, warning=FALSE}
library(ngram)

train_full$wordcount <- sapply(train_full$"Consumer complaint narrative", wordcount)
stats <- summary(train_full$wordcount)

ggplot() + aes(train_full$wordcount)+ geom_histogram(binwidth=10, colour="black", fill="white")+
  geom_vline(xintercept=stats[["Mean"]], color="green")+
  geom_vline(xintercept=c(stats[["1st Qu."]],stats[["3rd Qu."]]), color="red")+
  scale_x_continuous(limits = c(0, 500))
```

You can see that generally the complaints are fairly short (<150 words), but the distribution leads to much longer complaints - upwards of 500 words long.

```{r}
train <- train_full[(train_full$wordcount >= stats[["1st Qu."]] & train_full$wordcount <= stats[["3rd Qu."]]),]
train <- sample_n(train, 15000)
train$id <- c(1:nrow(train))
```

\newpage

## Word analysis 

Building a corpus, which is a collection of text documents VectorSource specifies that the source is character vectors.After that, the corpus needs a couple of transformations, including changing letters to lower case, removing punctuations/numbers and removing stop words. The general English stop-word list is tailored by adding some words specific to the documents in question.

```{r message=FALSE, warning=FALSE, linewidth = 90}
if(!file.exists("./user-complaints-mining/myCorpus.Rds")) {
  myCorpus <- Corpus(VectorSource(train$`Consumer complaint narrative`))
  myCorpus <- tm_map(myCorpus, removePunctuation)
  myCorpus <- tm_map(myCorpus, removeNumbers)
  myCorpus <- tm_map(myCorpus, tolower)
  myStopwords <- c(stopwords(language="en", source="smart"), 
                  "xx", "xxxx", "xxxxxxxxxxxx", "xxxxxxxx",
                  "told", "well", "month", "year"
                   )
  myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
  myCorpus <- tm_map(myCorpus, stripWhitespace)
  saveRDS(myCorpus, file = "./user-complaints-mining/myCorpus.Rds")
  gc()
} else {
  myCorpus <- readRDS("./user-complaints-mining/myCorpus.Rds")
}
print(myCorpus[1:5]$content)
```


### Steming
```{r message=FALSE, warning=FALSE, linewidth = 90}
dictCorpus <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
print(myCorpus[1:5]$content)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, linewidth=90}
### Stem completion - failed on out of memory, skipping for now
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
print(myCorpus[1:5]$content)
```
\newpage
# Building a Document-Term Matrix

This operation is resource and time consuming. To avoid calculation, the pre-build myDtm object will be loaded from the file system. To recalculate it needs to be removed from the file system first.

```{r message=FALSE, warning=FALSE}
if(!file.exists("./user-complaints-mining/myDtm.Rds")) {
  myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
  rowTotals <- apply(myDtm , 1, sum) #Find the sum of words in each Document
  myDtm <- myDtm[rowTotals > 0, ] #remove all docs without words
  #myDtm <- removeSparseTerms(myDtm, sparse = 0.99)
  saveRDS(myDtm, file = "./user-complaints-mining/myDtm.Rds")
  gc()
} else {
  myDtm <- readRDS("./user-complaints-mining/myDtm.Rds")
}
inspect(myDtm)
```
Some of the more common terms - Credit, report, debt - are obvious given this is a set of complaints about financial institutions 

\newpage
## Frequent Terms and Association
```{r}
freq.terms <- findFreqTerms(myDtm, lowfreq=5)
term.freq <- rowSums(as.matrix(myDtm))
term.freq <- subset(term.freq, term.freq >= 5)
```

```{r figMFT, fig.height=5, fig.width=5.5, message=FALSE, warning=FALSE, fig.cap="30 Most Frequent Terms"}
dfTerms <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(dfTerms[order(-dfTerms$freq),][1:30,], aes(x = reorder(term, freq), y = freq)) + 
  geom_bar(stat = "identity") + xlab("Terms") + ylab("") + coord_flip()
```

## Which words are associated with term "loan"?
```{r}
findAssocs(myDtm, c('loan'), 0.3)
```
There are no surprises here for the words associated with 'Loan'. Student, payment and interest are quite obvious. 

\newpage 

## Building word cloud:

```{r figCl1, fig.height=5.5, fig.width=5.5, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Words Cloud of Complaints"}
library(wordcloud)
m <- as.matrix(myDtm)
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, min.freq=20, scale=c(4,.2), max.words = 200)
```

This is a more graphical representation of the horizontal bar chart shown above. 

\newpage

## Building LDA model for the 'train' set of complaints

```{r message=FALSE, warning=FALSE}
library(topicmodels)
dtm <- as.DocumentTermMatrix(myDtm)
numTopics <- 12 # 7, 10 or 30
topicsFileName <- paste("./user-complaints-mining/lda",numTopics,".Rds", sep = "") 
ui = unique(dtm$i) #array of unique row ids in 'train' set
if(!file.exists(topicsFileName)) {
  dtm.new = dtm[ui,]
  train.lda <- LDA(dtm.new, k = numTopics) # identify topics
  saveRDS(train.lda, file = topicsFileName)
  gc()
} else {
  train.lda <- readRDS(topicsFileName)
}  
(train.terms <- terms(train.lda, 10)) # first terms of every topic
```

\newpage
## Show complaints correlation to topics in 'train' set

```{r message=FALSE, warning=FALSE, linewidth = 90}
train.topics <- topics(train.lda, 5, threshold=.005)
for (i in c(1,55,500,333)) {
  print (paste(" "))
  print (paste("Complaint# ", i))
  print(paste("Topic(s) found: ", train.topics[i]))
  print (train$`Consumer complaint narrative`[ui[i]])
}
```


\newpage

# Topics related to most frequent issues

## Function definitions
```{r}
prepareCorpus <- function(textArr) {
  myCorpus <- Corpus(VectorSource(textArr))
  myCorpus <- tm_map(myCorpus, removePunctuation)
  myCorpus <- tm_map(myCorpus, removeNumbers)
  myCorpus <- tm_map(myCorpus, tolower)
  myStopwords <- c(stopwords(language="en", source="smart"), 
                  "xx", "xxxx", "xxxxxxxxxxxx", "xxxxxxxx")
  myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
  myCorpus <- tm_map(myCorpus, stripWhitespace)
  
  return (myCorpus)
}

# Steming and reverse - long operation!
doSteming <- function(myCorpus) {
  #dictCorpus <- myCorpus
  myCorpus <- tm_map(myCorpus, stemDocument)
  #myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
  return (myCorpus)
}

getBestTopicNums <- function(arr, threshold) {
  topNums <- sort(arr, index.return=TRUE, decreasing = TRUE)$ix
  arr <- arr[arr>threshold]
  return (topNums[1:length(arr)])
}

```

\newpage

## Form a matrix of topic probability vectors for each issue

```{r}
issueTopicsProbMat <- matrix(NA,nrow = issuesToPredict, ncol = numTopics)

# for each issue
for (i in 1:issuesToPredict) {
  issueName <- full_sorted_issues_list[i]
  print(issueName)

  # matrix of issue topics probabilities
  issueids <- subset(train[,c("id")], train$Issue == issueName)
  uiIds <- match(issueids$id, ui)
  topicsProbabMatr <- train.lda@gamma[uiIds,] 
  meanVec <- colMeans(topicsProbabMatr, na.rm = T)
  issueTopicsProbMat[i,] <- meanVec
}
saveRDS(issueTopicsProbMat, file = "./user-complaints-mining/issueTopicsProbMat.Rds")
```


\newpage
## Identify issues of random 'test' complaints

```{r message=FALSE, warning=FALSE, linewidth = 90}
distInfo <- function(vec, mat) {
  retVec <- vector(length = nrow(mat))
  for (i in 1:nrow(mat)) {
    retVec[i] <- JSD(rbind(vec,mat[i,]), est.prob = "empirical")
  }
  retVec <- sort(retVec, index.return = TRUE)
  return (retVec)
}
# prepare test data
numOfTests <- 10000
numOfComplaintsToPrint <- 5
testThreshold =0.1
set.seed(123)
randomIds <- sample(nrow(test),numOfTests)
test.corpus <- prepareCorpus(test$`Consumer complaint narrative`[randomIds])
test.corpus <- doSteming(test.corpus)
test.dtm <- DocumentTermMatrix(test.corpus, control = list(minWordLength = 1))

#remove all docs with low number of words and correct supporting info
rowTotals  <- apply(test.dtm , 1, sum) #num words in each Document
test.dtm   <- test.dtm[rowTotals > 10,] 
numOfTests <- test.dtm$nrow
randomIds  <- randomIds[rowTotals > 10]
test.topics <- posterior(train.lda,test.dtm)

origCls <- replicate(numOfTests, NA)
infrCls <- replicate(numOfTests, NA)
for (i in 1:numOfTests) {
  issuesFound <- distInfo (test.topics$topics[i,],issueTopicsProbMat)
  tops <- getBestTopicNums(test.topics$topics[i,], testThreshold)

  # orig issue id
  origIssueTxt <- test[[randomIds[i],"Issue"]][1]
  origIssueId <- match(origIssueTxt, full_sorted_issues_list)
  origCls[i] <-  origIssueId 
  
  # inferred issue id
  if (origIssueId %in% issuesFound$ix[1:3]) {
    infrCls[i] <- origIssueId
  } else {
    infrCls[i] <- issuesFound$ix[1]
  }
  if (i <= numOfComplaintsToPrint) {
    print("----------")
    print (paste("Original Issue  : ", origIssueTxt))
    print (paste("Inferred Issue 1: ", full_sorted_issues_list[issuesFound$ix[1]]))
    print (paste("Inferred Issue 2: ", full_sorted_issues_list[issuesFound$ix[2]]))
    print (paste("Inferred Issue 3: ", full_sorted_issues_list[issuesFound$ix[3]]))
    print(paste("Complaint narrative:",test[[randomIds[i],"Consumer complaint narrative"]]))
  }
}
```

\newpage

## Issue prediction accuracy

```{r}
(tbl <- table(origCls,infrCls))
(sum(diag(tbl)))/numOfTests
```
\newpage

#Other Model Attempts

##Feature Engineering for Decision Tree & Random Forest
```{r message=FALSE, warning=FALSE, linewidth = 90}
#starting with original Train/Test split
#prepare Train & Test data
library(tidytext)
library(reshape)

#define sparcity allowance
sparce.pct<-0.90 #picked a low number so my computer could process dataset. With more memory, this number could be higher

dftrain<-train_full[-(8:9)]
dftest<-test[-8]

dftrain$train<-"y"
dftest$train<-"n"

colnames(dftest)==colnames(dftrain)


df3<-rbind(dftrain,dftest)

df3.corpus<-prepareCorpus(df3$`Consumer complaint narrative`)
df3.corpus <- doSteming(df3.corpus)
df3.dtm <- DocumentTermMatrix(df3.corpus, control = list(weighting=weightTfIdf, minWordLength = 1))
df3.dtm<-removeSparseTerms(df3.dtm,sparce.pct)

#put into dataframe using tidytext package
df3.dtm.tidy<-tidy(df3.dtm)
df3.dtm.tidy<-as.data.frame(df3.dtm.tidy)

#have to rename docID column to contain caps so we can do the left join later
colnames(df3.dtm.tidy)[1]<-"documentID"

#cast into dataframe (similar to pivot table)
df3.dtm.DF<-cast(df3.dtm.tidy, documentID ~ term)

head(df3.dtm.DF)


df3.dtm.DF$documentID<-as.integer(df3.dtm.DF$documentID)
df3.dtm.DF<-as.data.frame(df3.dtm.DF)
df3<-as.data.frame(df3)

#add a sequence to the original data in order to left-join to new dataset
df3$ID <- seq.int(nrow(df3))

#left join the original df3 to dtm df
df3.df<-left_join(df3, df3.dtm.DF, by = c("ID"="documentID"))

df3.df<-df3.df[,!names(df3.df) %in% c("Product","Sub-product","Sub-issue","Consumer complaint narrative","Company","issueId","ID","document")]



#convert anything that's not "top issue" to "Other"
most_freq_issues_list <- levels(fct_infreq(df$Issue))[1:30]
most_freq_issues_list<-as.list(most_freq_issues_list)
tmp2<-!df3.df$Issue %in% most_freq_issues_list
df3.df$Issue<-as.character(df3.df$Issue)
df3.df$Issue[tmp2]<-"Other Issue"
df3.df$Issue<-as.factor(df3.df$Issue)


#NA's to 0s
df3.df[is.na(df3.df)] <- 0

train.df<-df3.df[df3.df$train=="y",]
train.df<-train.df[-2]
test.df<-df3.df[df3.df$train=="n",]
test.df<-test.df[-2]
```

##Decision Tree
```{r message=FALSE, warning=FALSE, linewidth = 90}
library(C50)

## Control for C5.0 Model
C5.0Control(subset=TRUE,bands=2,winnow=TRUE,noGlobalPruning=FALSE,CF=0.25,minCases=2,label="C5.0 Outcome")## Train Model using C5.0

##Train the Data
train.c50=C5.0(train.df[,-which(colnames(train.df)=="Issue")],train.df$Issue,trials=10,rules=FALSE,control=C5.0Control(),costs=NULL)

##Prediction on Test Data and Train Data
testPred.c50=predict(object=train.c50,newdata=test.df[,-which(colnames(train.df)=="Issue")],trials=1,type="class")


confMat <- table(test.df$Issue, testPred.c50)
accuracy <- sum(diag(confMat)/sum(confMat))
print(accuracy) #41.3% Accuracy
```
Decision tree has a 41% accuracy. Not great.

Lets try 5-fold cross validation (my computer cannot converge with 10)

##5fold Cross Validation Decision Tree
```{r}
library(caret)
train_control <- trainControl(method = 'cv', number = 5)
dt.grid <- expand.grid(.cp=0.0)

dt_5k <- train(Issue ~ ., 
              data=train.df, 
              method = 'rpart', 
              trControl = train_control, 
              metric='Accuracy', 
              tuneGrid = dt.grid)

t_pred <- predict(dt_5k, test.df,type = 'raw')
confMat <- table(test.df$Issue, t_pred)
accuracy <- sum(diag(confMat)/sum(confMat))
print(accuracy) #40.7% Accuracy
```
40.7% accuracy -> lower than the decision tree. Likely to do with the tuning parameters. 

A random forest model was run in a separate file, with a resulting accuracy of 49.2%. This was run again within this file, but took too long to converge given the deadline to submit. The code below is for the model but is all commented out so the final report can be finalized. 

##Random Forest Model
```{r message=FALSE, warning=FALSE, linewidth = 90}

#library(party)
#test.dfp<-test.df[,!names(test.df) %in% c("Issue")]
#test.dfp[is.na(test.dfp)]<-0


#rf.grid <- expand.grid(.mtry=5.0)
#rf_Out_1 <- train(Issue ~ ., 
#              train.df, 
#              ntrees=150,
#              method = "rf", 
#             # trControl = train_control, 
#              tuneGrid = rf.grid)

#rf.pred<-predict(rf_Out_1, test.dfp, type = 'raw')

#confMat <- table(test.df$Issue, rf.pred)
#accuracy <- sum(diag(confMat)/sum(confMat))
#print(accuracy) #49% accuracy.
```


# Prepare data for use in Shiny App

## Save random test complaints from the set of 'issuesToPredict'
```{r}
complaintsToSave <- 2000 # will be about half of that after cleaning
set.seed(1234)
randomIds <- sample(nrow(test),complaintsToSave)
sampleComplaints <- test[randomIds, c("issueId","Issue","Consumer complaint narrative")]
sampleComplaints$wordcount <- sapply(sampleComplaints$"Consumer complaint narrative", wordcount)
stats <- summary(sampleComplaints$wordcount)
sampleComplaints <- sampleComplaints[
  (sampleComplaints$wordcount >= stats[["1st Qu."]] & 
     sampleComplaints$wordcount <= stats[["3rd Qu."]]),]
saveRDS(df, file = "./user-complaints-mining/sampleComplaints.Rds")
```



#Deployment
The long term deployment of this will be to channel complaints to appropriate product teams basied on the context of the complaint. Deploying it as an app such as this for a user to submit their complaint into will be sufficient.

New data can be collected with each complaint call. If the service desk clerk does not forward it off to someone else, then the label was appropriate. The model can be monitored with the number of "forwards" that come through the service desk clerks. If this number gets too high (10%?) then a model can be re-trained. 

##Link to Shiny App
https://ivbsoftware.shinyapps.io/Complainer/

